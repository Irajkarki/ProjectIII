{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838065be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 280ms/step\n",
      "[[5.87743070e-13 3.44381878e-14 3.34308220e-10 6.97164992e-10\n",
      "  2.36399408e-14 1.03572177e-06 1.18459302e-05 2.00252212e-03\n",
      "  9.97931361e-01 1.26608278e-16 5.45477178e-06 2.08766782e-09\n",
      "  1.11793526e-07 1.30489872e-11 1.04749085e-06 1.58116631e-08\n",
      "  1.12291287e-10 1.45734452e-06 5.07724852e-12 4.25350571e-14\n",
      "  5.62131564e-10 2.18158288e-19 3.09724766e-13 6.28533556e-15\n",
      "  7.86602639e-17 2.23148183e-16 4.31818454e-13 5.21920391e-14\n",
      "  1.13409906e-10 1.58642343e-18 1.55774005e-11 4.82108190e-12\n",
      "  6.65795624e-17 1.40728873e-13 2.93600526e-11 6.46789428e-11\n",
      "  3.73819686e-09 1.00641014e-16 1.52106421e-17 6.65728377e-13\n",
      "  2.00736894e-09 5.04361186e-23 4.07524407e-15 3.76787535e-09\n",
      "  2.72693970e-19 4.80014723e-14 4.80321134e-08 2.41244804e-16\n",
      "  1.33626568e-16 4.44130545e-15 1.64668423e-13 6.27381990e-13\n",
      "  2.61825026e-05 7.76030420e-11 5.51423447e-12 1.29147665e-10\n",
      "  3.92727006e-09 1.51187366e-15 1.97896860e-15 1.91783570e-07\n",
      "  1.74795169e-13 1.53905262e-06 5.64233993e-09 3.95417043e-13\n",
      "  2.50383643e-08 1.55270485e-09 5.85874450e-19 8.48329768e-17\n",
      "  3.09512721e-17 3.92441407e-18 1.16376504e-17 3.13718504e-16\n",
      "  1.25866745e-15 7.01584971e-12 2.39008276e-13 4.43372672e-09\n",
      "  1.24014757e-10 1.54271791e-13 3.25641071e-13 1.08588203e-07\n",
      "  2.30309063e-15 9.31927900e-18 1.98266574e-13 3.49511531e-11\n",
      "  8.36233454e-23 9.94036998e-16 7.89826248e-16 8.49554980e-22\n",
      "  4.84013107e-10 5.93667898e-18 7.79510358e-08 1.84757248e-16\n",
      "  3.29950567e-10 9.28739929e-09 6.09442374e-14 3.75603382e-10\n",
      "  1.31779096e-13 2.67318900e-17 6.33905515e-08 1.56442686e-12\n",
      "  2.41130469e-18 7.83736160e-17 3.00926749e-11 1.06756548e-11\n",
      "  1.40154029e-13 3.53415824e-14 3.39500164e-11 1.61744798e-25\n",
      "  8.54637270e-14 2.05307615e-09 3.10677817e-10 4.40305825e-17\n",
      "  7.27627070e-10 7.16286497e-09 2.67679301e-08 7.00917724e-08\n",
      "  1.94004188e-10 1.87639848e-13 1.60397791e-12 4.44349668e-09\n",
      "  1.72010549e-15 7.11003622e-06 1.10884761e-12 1.73136933e-15\n",
      "  3.69078205e-11 8.70809984e-08 2.42902684e-06 7.89100767e-08\n",
      "  1.28809139e-14 2.36034747e-08 8.46557616e-11 1.19261062e-14\n",
      "  6.78832750e-13 3.93059813e-20 9.97850999e-13 1.64535871e-10\n",
      "  1.52540977e-07 2.90932545e-15 1.47288015e-10 3.98038473e-08\n",
      "  7.46779381e-19 8.09559624e-12 4.27239547e-17 3.37228227e-12\n",
      "  1.76098811e-18 3.94417200e-22 2.79765612e-16 8.45870886e-07\n",
      "  1.77688175e-08 7.40008055e-10 2.88040669e-13 1.75252310e-08\n",
      "  4.81189524e-08 1.77800052e-06 1.07694506e-10 3.49114657e-06\n",
      "  2.86255232e-13 4.02033934e-10 1.46731082e-07 4.04758717e-16\n",
      "  5.36213520e-07 6.55443527e-13]]\n",
      "(1, 162)\n",
      "8\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "[[4.11965546e-13 2.71343684e-14 4.38842795e-10 7.83368537e-10\n",
      "  1.86327326e-14 1.37563586e-06 8.73299359e-06 1.63861585e-03\n",
      "  9.98308539e-01 1.22124099e-16 4.79895061e-06 1.85441107e-09\n",
      "  1.45403320e-07 1.36703478e-11 1.09501968e-06 1.49721551e-08\n",
      "  1.01580147e-10 1.27583223e-06 6.20201026e-12 3.94968448e-14\n",
      "  7.55054186e-10 1.85167650e-19 2.35824950e-13 6.18371533e-15\n",
      "  8.01132430e-17 2.65517195e-16 4.76508535e-13 5.14313256e-14\n",
      "  7.47746171e-11 1.56234700e-18 1.42548516e-11 4.23139648e-12\n",
      "  9.01445484e-17 1.03815692e-13 2.75434276e-11 8.63557698e-11\n",
      "  3.99235933e-09 6.32508949e-17 2.42319252e-17 8.64509365e-13\n",
      "  2.25124097e-09 6.68435293e-23 4.78219767e-15 4.70367345e-09\n",
      "  2.46059866e-19 5.17193574e-14 3.81924323e-08 2.31146900e-16\n",
      "  1.44161090e-16 5.79702022e-15 2.58397118e-13 5.32503078e-13\n",
      "  1.89172570e-05 5.28481599e-11 6.85442762e-12 1.16074129e-10\n",
      "  3.12937387e-09 2.03906411e-15 1.35742896e-15 1.64240504e-07\n",
      "  1.45081511e-13 1.50601841e-06 4.47542803e-09 4.44068393e-13\n",
      "  2.71922183e-08 1.46318457e-09 4.69040772e-19 9.20175764e-17\n",
      "  4.67761934e-17 4.54768019e-18 1.11374352e-17 1.61006842e-16\n",
      "  1.43766299e-15 5.12849355e-12 1.77821340e-13 3.48891893e-09\n",
      "  1.17529320e-10 1.01349315e-13 2.27964620e-13 1.17094068e-07\n",
      "  2.02554356e-15 9.79039558e-18 1.90092327e-13 2.69942072e-11\n",
      "  9.30394795e-23 9.94860419e-16 1.11190650e-15 9.05599899e-22\n",
      "  4.12453627e-10 7.20748358e-18 7.59311121e-08 1.60096136e-16\n",
      "  4.01504496e-10 8.80644446e-09 4.80873479e-14 4.18797830e-10\n",
      "  8.08114968e-14 2.10061309e-17 3.64652344e-08 1.97968161e-12\n",
      "  2.53064369e-18 7.87608623e-17 2.33614517e-11 1.37302149e-11\n",
      "  2.02094567e-13 3.71751139e-14 3.76688056e-11 1.58884080e-25\n",
      "  1.22827895e-13 2.23868635e-09 4.42416298e-10 5.14936707e-17\n",
      "  7.14147297e-10 8.60983107e-09 3.06960324e-08 6.50205720e-08\n",
      "  2.18750601e-10 2.10223671e-13 1.71049644e-12 4.41256098e-09\n",
      "  1.10123113e-15 6.88161754e-06 1.09399751e-12 1.54887884e-15\n",
      "  5.47449967e-11 8.88053151e-08 1.99217652e-06 6.79558951e-08\n",
      "  1.21842699e-14 3.05447685e-08 1.01491260e-10 1.02090780e-14\n",
      "  6.92353781e-13 3.54646482e-20 9.15183836e-13 2.41029419e-10\n",
      "  1.22080536e-07 3.06665441e-15 1.09163081e-10 5.21810577e-08\n",
      "  6.19943331e-19 1.07419993e-11 3.33135907e-17 2.27857381e-12\n",
      "  1.49049488e-18 4.60238365e-22 4.80060105e-16 7.80967866e-07\n",
      "  1.92246272e-08 4.61705035e-10 2.63877272e-13 1.82566886e-08\n",
      "  5.57269573e-08 1.45101410e-06 1.28863004e-10 2.27429973e-06\n",
      "  2.24030064e-13 3.02295605e-10 1.15995512e-07 3.23082480e-16\n",
      "  5.37555081e-07 6.83145218e-13]]\n",
      "(1, 162)\n",
      "8\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "[[6.64971767e-13 4.10647062e-14 2.00558750e-10 6.41270426e-10\n",
      "  3.34762362e-14 6.21359561e-07 9.61118985e-06 8.63331254e-04\n",
      "  9.99088883e-01 1.15944927e-16 6.07586389e-06 1.50481760e-09\n",
      "  9.66483071e-08 8.18582008e-12 7.40431744e-07 1.53103397e-08\n",
      "  1.35893671e-10 1.18161779e-06 5.03929147e-12 4.29440995e-14\n",
      "  6.40556497e-10 2.30500107e-19 3.71714243e-13 8.91103479e-15\n",
      "  1.02511686e-16 2.47528729e-16 4.83729701e-13 6.93788627e-14\n",
      "  7.56435747e-11 1.95495432e-18 2.09351043e-11 3.94045300e-12\n",
      "  5.33473493e-17 1.20795152e-13 4.06700472e-11 7.59270355e-11\n",
      "  2.81346169e-09 1.56290192e-16 2.19171066e-17 9.03591709e-13\n",
      "  2.06101647e-09 7.24743742e-23 2.68221008e-15 2.54187205e-09\n",
      "  3.85900203e-19 5.15625174e-14 2.52876848e-08 2.72742121e-16\n",
      "  1.06296428e-16 3.47132906e-15 1.42726204e-13 7.90629064e-13\n",
      "  1.66360205e-05 7.51695234e-11 6.17349184e-12 1.53757895e-10\n",
      "  3.60855501e-09 1.46461430e-15 2.36301842e-15 1.17172632e-07\n",
      "  1.47406311e-13 8.97278539e-07 5.86158810e-09 5.05079702e-13\n",
      "  2.01209165e-08 1.09310794e-09 6.23597970e-19 1.17261534e-16\n",
      "  2.52790316e-17 6.42723679e-18 8.81446390e-18 2.41408572e-16\n",
      "  1.39033957e-15 5.66373164e-12 1.40222672e-13 4.25473257e-09\n",
      "  1.46442830e-10 2.39101192e-13 3.78262742e-13 9.81201254e-08\n",
      "  2.20570302e-15 1.27295346e-17 2.76461173e-13 2.29420302e-11\n",
      "  8.32623500e-23 8.16257276e-16 1.06301343e-15 1.09839518e-21\n",
      "  2.37701053e-10 6.51223414e-18 9.07109836e-08 2.15946987e-16\n",
      "  2.16084442e-10 1.18600596e-08 7.62789083e-14 5.67997593e-10\n",
      "  1.17570762e-13 3.91651789e-17 2.40014746e-08 1.68110122e-12\n",
      "  3.94980935e-18 9.80067016e-17 3.92877050e-11 1.14616927e-11\n",
      "  7.37181651e-14 3.79229119e-14 2.06300046e-11 2.84772131e-25\n",
      "  8.26759315e-14 1.37695588e-09 3.82254534e-10 2.38237612e-17\n",
      "  6.79841627e-10 5.57797453e-09 1.84266984e-08 6.07407316e-08\n",
      "  1.31206421e-10 1.71553256e-13 2.45460596e-12 2.48323584e-09\n",
      "  1.33314463e-15 3.65780238e-06 1.09476913e-12 1.58169374e-15\n",
      "  4.43024610e-11 7.30210701e-08 1.71423687e-06 5.43560148e-08\n",
      "  1.28530287e-14 2.44156446e-08 6.85790522e-11 1.09206933e-14\n",
      "  3.83964589e-13 9.76792253e-20 9.25215633e-13 1.15658143e-10\n",
      "  8.35343670e-08 2.91181128e-15 1.00264221e-10 1.54311870e-08\n",
      "  1.40428147e-18 7.17525345e-12 3.73885008e-17 2.65553530e-12\n",
      "  1.63805553e-18 3.79214575e-22 2.63220703e-16 5.83863653e-07\n",
      "  1.94122745e-08 9.72707026e-10 3.28307666e-13 2.21068515e-08\n",
      "  3.62666981e-08 1.12609757e-06 1.11053944e-10 3.32705145e-06\n",
      "  3.24458640e-13 2.94646835e-10 1.47500430e-07 5.07026775e-16\n",
      "  4.75584443e-07 7.78470929e-13]]\n",
      "(1, 162)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model('updated94.h5')\n",
    "\n",
    "# # Load the label mapping\n",
    "# with open('label_mapping.json', 'r') as f:\n",
    "#     label_mapping = json.load(f)\n",
    "\n",
    "# # Load the Haar Cascade classifier for face detection\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Initialize the webcam\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     # Capture frame-by-frame from the webcam\n",
    "#     ret, frame = video_capture.read()\n",
    "\n",
    "#     # Convert the frame to grayscale for face detection\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Detect faces in the frame\n",
    "#     faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "#     # Process each detected face\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         # Preprocess the face image\n",
    "#         face_image = frame[y:y+h, x:x+w]\n",
    "#         face_image = cv2.resize(face_image, (224, 224))\n",
    "#         face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "#         face_image = face_image.astype('float32') / 255.0\n",
    "#         face_image = np.expand_dims(face_image, axis=0)\n",
    "\n",
    "#         # Perform face recognition using the trained model\n",
    "#         predictions = model.predict(face_image)\n",
    "#         predicted_label = np.argmax(predictions)\n",
    "#         print(predictions)\n",
    "#         print(predictions.shape)\n",
    "#         print(predicted_label)\n",
    "\n",
    "#         # Map the predicted label to the corresponding identity\n",
    "# #         identity = label_mapping[str(predicted_label)]\n",
    "\n",
    "#         # Draw bounding box and label on the face\n",
    "#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, str(predicted_label), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "        \n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "#     # Exit the loop if 'q' is pressed\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the webcam and close the window\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88315cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 25 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002ED036B3280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "[[0.00623285 0.00629959 0.0061611  0.00618585 0.0061463  0.00609201\n",
      "  0.00626786 0.00620122 0.0061836  0.00619389 0.00621372 0.00618471\n",
      "  0.00609813 0.00612989 0.00617652 0.0061665  0.00619334 0.00620025\n",
      "  0.0061178  0.00614998 0.00620518 0.0062024  0.00614798 0.00619812\n",
      "  0.00630141 0.00631651 0.00614547 0.00622849 0.00618627 0.00616565\n",
      "  0.00618489 0.00611795 0.00613789 0.00615304 0.0060253  0.00621216\n",
      "  0.00614036 0.00628993 0.00610197 0.00623058 0.00615943 0.00607675\n",
      "  0.00608015 0.00609872 0.0061368  0.00610278 0.00605059 0.00601794\n",
      "  0.00626485 0.00615932 0.00610762 0.00615161 0.00603973 0.00620054\n",
      "  0.00622059 0.00628842 0.00629382 0.00613686 0.00634075 0.00635639\n",
      "  0.00608164 0.00618721 0.00622922 0.00613116 0.0061191  0.00614015\n",
      "  0.00607224 0.00627658 0.00629581 0.00628888 0.00609551 0.00616953\n",
      "  0.0060726  0.00620579 0.00611113 0.00632866 0.00624886 0.00619264\n",
      "  0.00617153 0.00614746 0.00624721 0.00615046 0.00620376 0.0062027\n",
      "  0.00606247 0.00610929 0.00602994 0.00618137 0.0060805  0.0061338\n",
      "  0.00624984 0.00613489 0.00614878 0.00624492 0.00619979 0.00629562\n",
      "  0.00600117 0.00618121 0.00619135 0.00610748 0.00607191 0.00610012\n",
      "  0.00633941 0.00612917 0.0062077  0.0061039  0.00616873 0.00609126\n",
      "  0.00598313 0.00617253 0.0062873  0.00606424 0.0061408  0.00614891\n",
      "  0.00623063 0.0061766  0.00621919 0.00624263 0.00628806 0.00617445\n",
      "  0.00619959 0.0061844  0.00609816 0.00615154 0.00626421 0.00622221\n",
      "  0.00614352 0.00623879 0.00611423 0.00621323 0.00616742 0.00610482\n",
      "  0.0060625  0.00612413 0.00598277 0.0060278  0.00614211 0.00605772\n",
      "  0.00621    0.00615013 0.0061965  0.00622997 0.00606533 0.00625564\n",
      "  0.0062687  0.00615386 0.00632829 0.00604031 0.00638592 0.00628616\n",
      "  0.0061814  0.00619274 0.00608689 0.00620235 0.00626271 0.00620977\n",
      "  0.00622542 0.0061847  0.00608077 0.00622091 0.00625884 0.00619022]]\n",
      "(1, 162)\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('updated94.h5')\n",
    "\n",
    "# Load the label mapping\n",
    "with open('label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Load the Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the input image\n",
    "image = cv2.imread('Lily_Tomlin.jpg')\n",
    "\n",
    "# Convert the image to grayscale for face detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Process each detected face\n",
    "for (x, y, w, h) in faces:\n",
    "    # Preprocess the face image\n",
    "    face_image = image[y:y+h, x:x+w]\n",
    "    face_image = cv2.resize(face_image, (224, 224))\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "    face_image = face_image.astype('float32') / 255.0\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "\n",
    "    # Perform face recognition using the trained model\n",
    "    predictions = model.predict(face_image)\n",
    "    predicted_label = np.argmax(predictions)\n",
    "    print(predictions)\n",
    "    print(predictions.shape)\n",
    "    print(predicted_label)\n",
    "\n",
    "    # Map the predicted label to the corresponding identity\n",
    "#     identity = label_maapping[str(predicted_label)]\n",
    "\n",
    "    # Draw bounding box and label on the face\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.putText(image, str(predicted_label), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the resulting image\n",
    "cv2.imshow('Face Recognition', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4447cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aaron_Tippin': 0, 'Abba_Eban': 1, 'Abbas_Kiarostami': 2, 'Abdel_Aziz_Al-Hakim': 3, 'Abdel_Madi_Shabneh': 4, 'Abdel_Nasser_Assidi': 5, 'Abdoulaye_Wade': 6, 'Abdulaziz_Kamilov': 7, 'Abdullah': 8, 'Abdullah_Ahmad_Badawi': 9, 'Abdullah_al-Attiyah': 10, 'Abdullah_Gul': 11, 'Abdullah_Nasseef': 12, 'Abdullatif_Sener': 13, 'Abdul_Majeed_Shobokshi': 14, 'Abdul_Rahman': 15, 'Abel_Aguilar': 16, 'Abel_Pacheco': 17, 'Abid_Hamid_Mahmud_Al-Tikriti': 18, 'Abner_Martinez': 19, 'Abraham_Foxman': 20, 'Aby_Har-Even': 21, 'Bill_Kong': 22, 'Bill_Lerach': 23, 'Bill_Maher': 24, 'Bill_Mauldin': 25, 'Bill_McBride': 26, 'Bill_Nelson': 27, 'Bill_OReilly': 28, 'Bill_Parcells': 29, 'Bill_Parsons': 30, 'Bill_Paxton': 31, 'Bill_Pryor': 32, 'Bill_Rainer': 33, 'Bill_Readdy': 34, 'Kweisi_Mfume': 35, 'Kwon_Yang-sook': 36, 'Kwon_Young-gil': 37, 'Kyle_McLaren': 38, 'Kyle_Shewfelt': 39, 'Kyoko_Nakayama': 40, 'Kyra_Sedgwick': 41, 'Lachlan_Murdoch': 42, 'Laila_Ali': 43, 'Lana_Clarkson': 44, 'Lance_Armstrong': 45, 'Lance_Bass': 46, 'Landon_Donovan': 47, 'Lane_Bryant': 48, 'Lane_Odom': 49, 'Lara_Logan': 50, 'Lee_Hong-ki': 51, 'Lee_Hyung-taik': 52, 'Lee_Jun': 53, 'Lee_Nam-shin': 54, 'Lee_Soo-hyuck': 55, 'Lee_Tae-sik': 56, 'Lee_Yeo-jin': 57, 'Lee_Yuan-tseh': 58, 'Leigh_Winchell': 59, 'Leisel_Jones': 60, 'Leland_Chapman': 61, 'Lela_Rochon': 62, 'Lemuel_Montulo': 63, 'Lena_Katina': 64, 'Lena_Olin': 65, 'Lene_Espersen': 66, 'Leni_Bjorklund': 67, 'Len_Jenoff': 68, 'Lidija_Djukanovic': 69, 'Liliana_Cavani': 70, 'Lili_Marinho': 71, 'Lili_Taylor': 72, 'Lily_Safra': 73, 'Lily_Tomlin': 74, 'Lima_Azimi': 75, 'Lim_Dong-won': 76, 'Lina_Krasnoroutskaya': 77, 'Lincoln_Chafee': 78, 'Linda_Amicangioli': 79, 'Linda_Baboolal': 80, 'Linda_Dano': 81, 'Linda_Franklin': 82, 'Linda_Ham': 83, 'Lin_Yi-fu': 84, 'Lin_Yung_Hsi': 85, 'Natasa_Micic': 86, 'Natasha_Henstridge': 87, 'Natasha_Lyonne': 88, 'Natasha_McElhone': 89, 'Nate_Blackwell': 90, 'Nate_Huffman': 91, 'Nate_Hybl': 92, 'Nathalia_Gillot': 93, 'Nathalie_Baye': 94, 'Nathalie_Dechy': 95, 'Nathalie_Gagnon': 96, 'Nathan_Doudney': 97, 'Nathan_Lane': 98, 'Nathan_Powell': 99, 'Nathan_Smith': 100, 'Nathirah_Hussein': 101, 'Nawabzada_Nasrullah_Khan': 102, 'Nebojsa_Pavkovic': 103, 'Raymond_Odierno': 104, 'Razali_Ismail': 105, 'Raza_Rabbani': 106, 'Rebecca_Romijn-Stamos': 107, 'Rebekah_Chantay_Revels': 108, 'Recep_Tayyip_Erdogan': 109, 'Red_Auerbach': 110, 'Reese_Witherspoon': 111, 'Reggie_Lewis': 112, 'Reggie_Miller': 113, 'Reggie_Sanders': 114, 'Reginald_Hudlin': 115, 'Regina_Ip': 116, 'Reina_Hayes': 117, 'Sally_Ride': 118, 'Salman_Khan': 119, 'Salman_Rushdie': 120, 'Salma_Hayek': 121, 'Samantha_Daniels': 122, 'Samantha_Ledster': 123, 'Saman_Shali': 124, 'Samira_Makhmalbaf': 125, 'Sami_Al-Arian': 126, 'Sammy_Knight': 127, 'Sammy_Sosa': 128, 'Samuel_Waksal': 129, 'Sam_Bith': 130, 'Sam_Brownback': 131, 'Sam_Gerald': 132, 'Sam_Mendes': 133, 'Sam_Rockwell': 134, 'Sam_Torrance': 135, 'Sananda_Maitreya': 136, 'Sandra_Banning': 137, 'Sandra_Bullock': 138, 'Sandra_Ceccarelli': 139, 'Sandra_Day_OConner': 140, 'Sandra_Milo': 141, 'Sandra_Shamas': 142, 'Sandy_Smith': 143, 'Sandy_Wise': 144, 'Sanja_Papic': 145, 'San_Lan': 146, 'Wayne_Gretzky': 147, 'Wayne_Newton': 148, 'Wei_Wu': 149, 'Wendell_Bryant': 150, 'Wendy_Kennedy': 151, 'Wendy_Selig': 152, 'Wen_Ho_Lee': 153, 'Wen_Jiabao': 154, 'Werner_Schlager': 155, 'Wesley_Clark': 156, 'Wes_Craven': 157, 'Wilbert_Elki_Meza_Majino': 158, 'Wilbert_Foy': 159, 'Wilfredo_Moreno': 160, 'Will_Ferrell': 161}\n",
      "1/1 [==============================] - 0s 247ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'74'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12072\\2943586203.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Map the predicted label to the corresponding class or identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mpredicted_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Print the predicted class or perform further processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '74'"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the pre-trained model\n",
    "# model = load_model('94best.h5')\n",
    "\n",
    "# # Load the label mapping from JSON\n",
    "# with open('labelCriminals.json', 'r') as f:\n",
    "#     label_mapping = json.load(f)\n",
    "# print(label_mapping)\n",
    "\n",
    "\n",
    "# # Load the image\n",
    "# image_path = 'Lily_Tomlin.jpg'\n",
    "# image = cv2.imread(image_path)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "# image = cv2.resize(image, (224, 224))  # Resize if necessary\n",
    "\n",
    "# # Preprocess the image\n",
    "# image = image.astype('float32') / 255.0\n",
    "# image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# # Perform the prediction\n",
    "# predictions = model.predict(image)\n",
    "\n",
    "# # Get the predicted label and class probability\n",
    "# predicted_label = np.argmax(predictions)\n",
    "# class_probability = np.max(predictions)\n",
    "\n",
    "# # Map the predicted label to the corresponding class or identity\n",
    "# predicted_class = label_mapping[str(predicted_label)]\n",
    "\n",
    "# # Print the predicted class or perform further processing\n",
    "# print(\"Predicted class:\", predicted_class)\n",
    "# print(\"Class probability:\", class_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2ae6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
